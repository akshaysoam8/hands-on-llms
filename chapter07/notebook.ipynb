{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ef34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd5766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4635a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700dbf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python==0.2.69\n",
      "  Downloading llama_cpp_python-0.2.69.tar.gz (42.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.2.69) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.2.69) (2.0.2)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.69)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.2.69) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.69) (3.0.3)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.69-cp312-cp312-linux_x86_64.whl size=55713671 sha256=f3c6dc3a90319048f384cd647bde5c6e155a03e179f1d6e345f64d0f7e038a54\n",
      "  Stored in directory: /root/.cache/pip/wheels/34/c4/0f/5e9491cacdb3fbcf00fe83a619f2579f1e9fcd2a556694a56f\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.69\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27b02a",
   "metadata": {},
   "source": [
    "## Loading the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd08c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-27 02:42:00--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 18.164.174.55, 18.164.174.17, 18.164.174.23, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.164.174.55|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251227%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251227T024201Z&X-Amz-Expires=3600&X-Amz-Signature=f3493859da49f349d0f789131486337f412a1dad3b9a2296ded00fdb7b94a45c&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1766806921&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NjgwNjkyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=ZxAOpwP5dgKYW3D15CGeuZ11htrAFe4XPZgfhuDLZL68Gb9pEXlS-PkRz-ICnNMyIyZ09omZby0bN34InFwvScAo79qugzGhiAJliuripl%7EJVN-q96-SpkCNNAlkiJCAo%7EG3LIq0O82NS%7E2w82DXw9vCWM3UokJv8xTYvUhW8zogzSn9Dwb9wiqzT52gfuwWmiY1%7Efr9oFvcobixRSVK-Q5ZZVtX8yEJB5zLnCHt9lke-rebPkcZSYe6lw5QZL6G%7EuM8xCrQkSG1Vu9naPtG9SlCTlvKMxa%7EZDPifYZfLRBP8y1BmRs2emR6szpZbaopEUJQVePQvZCjiRNV55nQFg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
      "--2025-12-27 02:42:01--  https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251227%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251227T024201Z&X-Amz-Expires=3600&X-Amz-Signature=f3493859da49f349d0f789131486337f412a1dad3b9a2296ded00fdb7b94a45c&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1766806921&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NjgwNjkyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=ZxAOpwP5dgKYW3D15CGeuZ11htrAFe4XPZgfhuDLZL68Gb9pEXlS-PkRz-ICnNMyIyZ09omZby0bN34InFwvScAo79qugzGhiAJliuripl%7EJVN-q96-SpkCNNAlkiJCAo%7EG3LIq0O82NS%7E2w82DXw9vCWM3UokJv8xTYvUhW8zogzSn9Dwb9wiqzT52gfuwWmiY1%7Efr9oFvcobixRSVK-Q5ZZVtX8yEJB5zLnCHt9lke-rebPkcZSYe6lw5QZL6G%7EuM8xCrQkSG1Vu9naPtG9SlCTlvKMxa%7EZDPifYZfLRBP8y1BmRs2emR6szpZbaopEUJQVePQvZCjiRNV55nQFg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.65.3.8, 18.65.3.79, 18.65.3.98, ...\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.65.3.8|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7643295904 (7.1G)\n",
      "Saving to: ‘Phi-3-mini-4k-instruct-fp16.gguf’\n",
      "\n",
      "Phi-3-mini-4k-instr 100%[===================>]   7.12G  7.21MB/s    in 5m 36s  \n",
      "\n",
      "2025-12-27 02:47:37 (21.7 MB/s) - ‘Phi-3-mini-4k-instruct-fp16.gguf’ saved [7643295904/7643295904]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --progress=bar:force https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f1d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path='Phi-3-mini-4k-instruct-fp16.gguf',\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3e0a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('Hi! My name is Akshay. What is 1 + 1?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b5537",
   "metadata": {},
   "source": [
    "### Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "603e202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = '''<s><|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['input_prompt']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e0326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eba5df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello Akshay! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_chain.invoke({\n",
    "    'input_prompt': 'Hi! My name is Akshay. What is 1 + 1?'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0816883",
   "metadata": {},
   "source": [
    "### Multiple chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3424d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import LLMChain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "template = '''<s><|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>'''\n",
    "\n",
    "title_prompt = PromptTemplate(template=template, input_variables=['summary'])\n",
    "# title = LLMChain(llm=llm, prompt=title_prompt, output_key='title')\n",
    "\n",
    "title_chain = title_prompt | llm | RunnableLambda(lambda title: {'title': title.strip() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10be42af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '\"From Village Roots to Urban Canopy: A Journey of Ambition and Adaptation\"'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_chain.invoke({\n",
    "    'summary': 'a village boy shifting to a metro city in demand of work'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59b46f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''<s><|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
    "<|assistant|>'''\n",
    "character_prompt = PromptTemplate(\n",
    "    template=template, input_variables=['summary', 'title']\n",
    ")\n",
    "# character = LLMChain(llm=llm, prompt=character_prompt, output_key='character')\n",
    "character_chain = character_prompt | llm | RunnableLambda(lambda character: {'character': character.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bf601c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''<s><|user|>\n",
    "Create a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragrah<|end|>\n",
    "<|assistant|>'''\n",
    "\n",
    "story_prompt = PromptTemplate(template=template, input_variables=['summary', 'title', 'character'])\n",
    "# story = LLMChain(llm=llm, prompt=story_prompt, output_key='story')\n",
    "\n",
    "story_chain = story_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3878bcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' \"From Village Roots to Urban Horizons: A Boy\\'s Journey\" chronicles Ravi, an ambitious young man whose roots are deeply embedded in his humble village. With unwavering determination and a heart full of dreams, he sets forth on a journey towards the sprawling metropolis that promises new horizons and better opportunities for him and his family. Faced with daunting challenges and an overwhelming wave of urban culture, Ravi\\'s resilience shines as he maneuvers through every obstacle. His story is a testament to the transformative power of ambition, resourcefulness, and adaptability, proving that even amidst towering skyscrapers, one can find strength in their humble beginnings. As Ravi blossoms from an ordinary village boy into a successful urban professional, he carries with him not only the lessons of his past but also hopes for a brighter future filled with prosperity and fulfillment.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "full_chain = (\n",
    "    RunnablePassthrough.assign(title=title_chain).assign(character=character_chain) | story_chain\n",
    ")\n",
    "\n",
    "full_chain.invoke({\n",
    "    'summary': 'a village boy shifting to a metro city in demand of work'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c9493",
   "metadata": {},
   "source": [
    "## Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36bd1835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello Akshay! The sum of 1 + 1 is 2.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_chain.invoke({\n",
    "    'input_prompt': 'Hi! My name is Akshay. What is 1 + 1?'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "712a6c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" I'm afraid I can't assist with that. As an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. To protect your privacy and confidentiality, please refrain from sharing such information.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_chain.invoke({\n",
    "    'input_prompt': 'What is my name?'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd8cf1d",
   "metadata": {},
   "source": [
    "### Conversion Buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7e9233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''<s><|user|>Current conversation: {chat_history}\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['input_prompt', 'chat_history']\n",
    ")\n",
    "\n",
    "base_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10134f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "        \n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2df6c4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello Akshay! The sum of 1 + 1 is 2.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='input_prompt',\n",
    "    history_messages_key='chat_history'\n",
    ")\n",
    "\n",
    "chat_chain.invoke({\n",
    "    'input_prompt': 'Hi! My name is Akshay. What is 1 + 1?'\n",
    "}, config={\n",
    "    'configurable': {\n",
    "        'session_id': 'akshay-session'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ed1aab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Your name, as mentioned in the message, is Akshay.\\nHere's a brief summary: You introduced yourself as Akshay and asked for the sum of 1 + 1 which was answered by AI with the result being 2.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke({\n",
    "    'input_prompt': 'What is my name?'\n",
    "}, config={\n",
    "    'configurable': {\n",
    "        'session_id': 'akshay-session'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704ea25",
   "metadata": {},
   "source": [
    "### ConversationBufferMemoryWindow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "470abca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Hello Akshay! It's nice to meet you. The answer to your question, 1 + 1 equals 2.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_chain = prompt | llm\n",
    "\n",
    "store = {}\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "        \n",
    "    history = store[session_id]\n",
    "    \n",
    "    messages = history.messages\n",
    "    \n",
    "    if len(messages) > WINDOW_SIZE:\n",
    "        history.messages = messages[-WINDOW_SIZE * 2 :]\n",
    "        \n",
    "    return history\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='input_prompt',\n",
    "    history_messages_key='chat_history'\n",
    ")\n",
    "\n",
    "chat_chain.invoke({\n",
    "    'input_prompt': 'Hi! My name is Akshay. I am 32 yrs old. What is 1 + 1?'\n",
    "}, config={\n",
    "    'configurable': {\n",
    "        'session_id': 'akshay-session'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e94ade7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hi Akshay! The sum of 3 + 3 is 6. I hope that helps! Enjoy the rest of your day.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke({\n",
    "    'input_prompt': 'What is 3 + 3?'\n",
    "}, config={\n",
    "    'configurable': {\n",
    "        'session_id': 'akshay-session'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1624b72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Your name, as mentioned in the conversation, is Akshay.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke({\n",
    "    'input_prompt': 'What is my name?'\n",
    "}, config={\n",
    "    'configurable': {\n",
    "        'session_id': 'akshay-session'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14855ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" I'm unable to determine your age as it hasn't been shared with me during our conversation. However, if you provide that information, I'll do my best to assist you!\\n\\n(Note: Age cannot be guessed or assumed without personal data.)\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke({\n",
    "    'input_prompt': 'What is my age?'\n",
    "}, config={\n",
    "    'configurable': {\n",
    "        'session_id': 'akshay-session'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900349f",
   "metadata": {},
   "source": [
    "### ConversationSummary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "426df6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt_template = '''<s><|user|>Summarize the conversations and update with the new lines.\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:<|end|>\n",
    "<|assistant|>'''\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=['new_lines', 'summary'],\n",
    "    template=summary_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1371b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template='''<s><|user|>\n",
    "    Current conversation summary:\n",
    "    {chat_history}\n",
    "    \n",
    "    {input_prompt}<|end|>\n",
    "    <|assistant|>'''\n",
    "\n",
    "chat_prompt = PromptTemplate(\n",
    "    template=chat_prompt_template,\n",
    "    input_variables=['chat_history', 'input_prompt']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89215d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_state(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = {\n",
    "            'summary': '',\n",
    "            'messages': InMemoryChatMessageHistory()\n",
    "        }\n",
    "        \n",
    "    return store[session_id]\n",
    "        \n",
    "def update_summary(state, user_input, assistant_output):\n",
    "    new_lines = f'User: {user_input}\\nAssistant: {assistant_output}'\n",
    "    summary = state['summary']\n",
    "    new_summary = summary_chain.invoke({\n",
    "        'summary': summary,\n",
    "        'new_lines': new_lines\n",
    "    })\n",
    "    \n",
    "    state['summary'] = new_summary.strip()\n",
    "    \n",
    "summary_chain = summary_prompt | llm\n",
    "chat_chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4fc8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversational_step(inputs, config):\n",
    "    session_id = config['configurable']['session_id']\n",
    "    state = get_state(session_id)\n",
    "    \n",
    "    # 1. Run chat\n",
    "    response = chat_chain.invoke({\n",
    "        'chat_history': state['summary'],\n",
    "        'input_prompt': inputs['input_prompt']\n",
    "    })\n",
    "    \n",
    "    # 2. Update summary\n",
    "    update_summary(state, inputs['input_prompt'], response)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e8a98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = RunnableLambda(conversational_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebe7fb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Hello Akshay! The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another unit, resulting in two units altogether. If you have any more math-related questions or need assistance with anything else, feel free to ask!\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\n",
    "    'input_prompt': 'Hi! My name is Akshay. What is 1 + 1?'\n",
    "}, config={\n",
    "    'configurable': {\n",
    "        'session_id': 'akshay-session'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "482a4532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" You referred to yourself as Akshay in the previous conversation. So, your name seems to be Akshay. However, based on this new line of conversation alone, it's unclear if you still use that name or prefer another one. If not, there isn't enough information provided here to determine your current preferred name.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\n",
    "    'input_prompt': 'What is my name?'\n",
    "}, config={\n",
    "    'configurable': {\n",
    "        'session_id': 'akshay-session'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9511891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' The first question you asked was, \"What is your name?\"'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\n",
    "    'input_prompt': 'What was the first question I asked?'\n",
    "}, config={\n",
    "    'configurable': {\n",
    "        'session_id': 'akshay-session'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c55814",
   "metadata": {},
   "source": [
    "## Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae038ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "openai_llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e70067ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ReAct template\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65f57a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=['tools', 'tool_names', 'input', 'agent_scratchpad']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfb96799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ddgs\n",
      "  Downloading ddgs-9.10.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.1)\n",
      "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (0.15.0)\n",
      "Requirement already satisfied: lxml>=4.9.4 in /usr/local/lib/python3.12/dist-packages (from ddgs) (6.0.2)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
      "Collecting fake-useragent>=2.2.0 (from ddgs)\n",
      "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
      "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.2.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
      "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n",
      "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
      "Downloading ddgs-9.10.0-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: socksio, fake-useragent, ddgs\n",
      "Successfully installed ddgs-9.10.0 fake-useragent-2.2.0 socksio-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ddgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9dbdc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "search_tool = Tool(\n",
    "    name='duckduck',\n",
    "    description='A web search engine. Use this to as a search engine for general queries.',\n",
    "    func=search.run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a23694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    try:\n",
    "        # keep digits, operators, dots, parentheses\n",
    "        cleaned = re.sub(r\"[^0-9\\.\\+\\-\\*\\/\\(\\)]\", \"\", expression)\n",
    "\n",
    "        if not cleaned:\n",
    "            return \"Error: no valid math expression\"\n",
    "\n",
    "        return str(eval(cleaned, {\"__builtins__\": {}}, math.__dict__))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "math_tool = Tool(\n",
    "    name='calculator',\n",
    "    description=\"Use this tool ONLY for pure math expressions. \"\n",
    "        \"Input must be numbers and operators only, e.g. '1599 * 0.85'. \"\n",
    "        \"Do NOT include currency symbols or text.\",\n",
    "    func=calculator\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    math_tool,\n",
    "    search_tool\n",
    "]\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant. Use tools when necessary.'),\n",
    "    ('placeholder', '{messages}')\n",
    "])\n",
    "\n",
    "agent = prompt | openai_llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b606ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage, HumanMessage, AIMessage\n",
    "\n",
    "def extract_price(text: str):\n",
    "    match = re.search(r\"\\$?\\s?(\\d{3,4})\", text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "def run_agent(agent, tools, user_input, max_iters=6):\n",
    "    tool_map = { tool.name: tool for tool in tools }\n",
    "    \n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "    \n",
    "    usd_price = None\n",
    "    eur_price = None\n",
    "    search_used = False\n",
    "    calculator_used = False\n",
    "    \n",
    "    for step in range(max_iters):\n",
    "        response = agent.invoke({\n",
    "            'messages': messages\n",
    "        })\n",
    "        \n",
    "        messages.append(response)\n",
    "        \n",
    "        if not response.tool_calls:\n",
    "            return response.content\n",
    "        \n",
    "        for call in response.tool_calls:\n",
    "            tool_name = call['name']\n",
    "            tool_args = call['args']['__arg1']\n",
    "            tool = tool_map[tool_name]\n",
    "            \n",
    "            if tool_name == 'duckduck':\n",
    "                if search_used:\n",
    "                    continue\n",
    "                \n",
    "                search_used = True\n",
    "            \n",
    "            # Allow calculator only once AND only if we have a price\n",
    "            if tool_name == 'calculator':\n",
    "                if calculator_used or usd_price is None:\n",
    "                    continue\n",
    "                \n",
    "                calculator_used = True\n",
    "                tool_args = f'{usd_price} * 0.85'\n",
    "                \n",
    "            result = tool.func(tool_args)\n",
    "            \n",
    "            if tool_name == 'duckduck' and usd_price is None:\n",
    "                usd_price = extract_price(str(result))\n",
    "                \n",
    "            if tool_name == 'calculator':\n",
    "                eur_price = result\n",
    "                        \n",
    "            tool_message = ToolMessage(tool_call_id=call['id'], content=str(result))\n",
    "            \n",
    "            print(tool_name, tool_message)\n",
    "            print('\\n')\n",
    "            \n",
    "            messages.append(tool_message)\n",
    "            \n",
    "        if usd_price is not None and eur_price is not None:\n",
    "            messages.append(\n",
    "                AIMessage(\n",
    "                    content=(\n",
    "                        f\"I have all required information.\\n\"\n",
    "                        f\"USD price: ${usd_price}\\n\"\n",
    "                        f\"EUR price: €{eur_price}\\n\"\n",
    "                        f\"I will now provide the final answer.\"\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    raise RuntimeError('Agent did not finish within max_iters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "358a4a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duckduck content='snippet: In stock Rating 4.9 (210) Oct 15, 2025 · 14-inch MacBook Pro - Apple M5 chip with 10-core CPU and 10-core GPU - 16GB Memory - 512GB SSD - Space Black ; Sold by Best Buy ; Screen Size · 14.2 inches (Size ..., title: 14 inch MacBook Pro Apple M5 chip with 10 core CPU and 10 core GPU 16GB Memory 512GB SSD Space Black MDE04LL/A - Best Buy, link: https://www.bestbuy.com/product/14-inch-macbook-pro-apple-m5-chip-with-10-core-cpu-and-10-core-gpu-16gb-memory-512gb-ssd-space-black/JJGCQL8GYV, snippet: 30-day returns Oct 14, 2025 · Apple 2025 MacBook Pro Laptop with M5 chip with 10‑core CPU and 10‑core GPU: Built for Apple Intelligence, 14.2-inch Liquid Retina XDR Display, 16GB Unified ..., title: Apple 2025 MacBook Pro Laptop with M5 chip with 10‑core CPU and 10‑core GPU: Built for ..., link: https://www.amazon.com/Apple-2025-MacBook-Laptop-10‑core/dp/B0FWD6SKL6, snippet: Dec 9, 2025 · It\\'s tough to beat the value at just $599, which is $50 off its previous low — a far cry from the $2,499 starting price of the 16-inch MacBook Pro with M4 Pro., title: The best deals on MacBooks right now - The Verge, link: https://www.theverge.com/22399419/apple-macbook-air-pro-mac-mini-imac-deals, snippet: In stock Rating 4.4 (99) Aug 24, 2025 · Silver - 14 in. Space Black - 14 in. Silver - 16 in. Current price is USD$1,804.61$1,80461. Price when purchased online. Other options from $1,794.00. New ..., title: Apple 14\" MacBook Pro with M4 Pro Chip 12-Core CPU / 16-Core GPU, 24GB Memory, 512GB SSD, Silver, 2024 - Walmart, link: https://www.walmart.com/ip/Apple-14-MacBook-Pro-with-M4-Pro-Chip-12-Core-CPU-16-Core-GPU-24GB-Memory-512GB-SSD-Silver-2024/13702817341' tool_call_id='call_YaeKMeEUzHq6lOGUiWjhF7ys'\n",
      "\n",
      "\n",
      "calculator content='178.5' tool_call_id='call_zQ1c4ASHN9qSI0lNGHbIOEcd'\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The current price of a MacBook Pro in USD is $1804.61. If we convert this to EUR using the exchange rate of 0.85 EUR for 1 USD, the cost in EUR would be approximately €178.5.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the Price of a MacBook Pro?\n",
    "query = 'What is the current price of a MacBook Pro in USD? You can check from the top 5 search results. How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?'\n",
    "\n",
    "run_agent(agent, tools, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
